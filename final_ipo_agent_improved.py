"""
Agente IPO Final Melhorado
- Corre√ß√£o ortogr√°fica antes do agrupamento
- An√°lise inteligente de sentido
- Relat√≥rio detalhado como modelo fornecido
"""

import pandas as pd
import os
from datetime import datetime
from typing import Dict, List, Any, Tuple

from improved_coding_system import ImprovedIPOCodingSystem

class FinalIPOAgentImproved:
    """Agente IPO final com sistema melhorado"""
    
    def __init__(self):
        self.coding_system = ImprovedIPOCodingSystem()
    
    def analyze_question_type(self, data: list) -> str:
        """
        Analisa o tipo de quest√£o conforme regras do IPO:
        - Fechada: Apenas c√≥digos num√©ricos (exceto 98/99 etc que s√£o NS/NR)
        - Semi-aberta: Mistura de c√≥digos num√©ricos e texto
        - Aberta: Predomin√¢ncia de texto
        """
        total = len(data)
        if total == 0:
            return "vazia"
            
        numeric_count = 0
        text_count = 0
        
        for item in data:
            try:
                # Tenta converter para float/int
                float(item)
                numeric_count += 1
            except (ValueError, TypeError):
                # Se tem texto e n√£o √© vazio/espa√ßo
                if str(item).strip():
                    text_count += 1
                    
        # Regras de decis√£o IPO
        if text_count == 0 and numeric_count > 0:
            return "fechada"
        elif text_count > 0 and numeric_count > 0:
            return "semi-aberta"
        else:
            return "aberta"

    def process_single_question_with_chatgpt(self, question_data: list, existing_codes: dict, question_name: str) -> dict:
        print(f"[DEBUG] Entrou em process_single_question_with_chatgpt para: {question_name}", flush=True)
        
        # 1. An√°lise do Tipo de Quest√£o (L√≥gica IPO)
        q_type = self.analyze_question_type(question_data)
        print(f"[DEBUG] Tipo de quest√£o detectado: {q_type.upper()}", flush=True)
        
        if q_type == "fechada":
            # Quest√µes fechadas n√£o passam pelo GPT
            return {
                'question_name': question_name,
                'total_responses': len(question_data),
                'valid_responses': len(question_data),
                'existing_codes': existing_codes,
                'final_codes': existing_codes,
                'new_codes': {},
                'groups': {},
                'detailed_report': "Quest√£o identificada como FECHADA. Nenhum processamento de IA necess√°rio.",
                'code_column': question_data, # Retorna os pr√≥prios dados como c√≥digos
                'response_column': question_data,
                'processing_method': 'ignorado_fechada',
                'statistics': {'total_codes': 0, 'new_codes_count': 0, 'groups_with_multiple': 0, 'largest_group_size': 0}
            }

        # Para semi-abertas, filtramos apenas os textos para enviar ao GPT
        items_to_process = []
        indices_to_process = []
        
        if q_type == "semi-aberta":
            print("[DEBUG] Processando como SEMI-ABERTA: filtrando apenas textos.", flush=True)
            for i, item in enumerate(question_data):
                is_text = False
                try:
                    float(item)
                except ValueError:
                    if str(item).strip():
                        is_text = True
                
                if is_text:
                    items_to_process.append(item)
                    indices_to_process.append(i)
        else:
            # Aberta: processa tudo (exceto vazios ou c√≥digos de NS/NR se estiverem misturados, mas assumimos tudo como texto)
            items_to_process = question_data
            # Mapeamento 1:1
        
        # OTIMIZA√á√ÉO IPO: Extrai respostas √∫nicas para criar o Codebook
        # Isso evita enviar respostas repetidas e garante que o modelo foque em criar categorias
        unique_items = sorted(list(set([str(x).strip() for x in items_to_process if str(x).strip()])))
        
        print(f"[DEBUG] Dados para processar na IA: {len(items_to_process)} itens totais -> {len(unique_items)} itens √öNICOS", flush=True)

        print(f"[DEBUG] Dados recebidos: question_data={len(question_data)} itens, existing_codes={len(existing_codes)}", flush=True)
        # Converte c√≥digos existentes para lista de strings para o prompt
        f17_list = [f"{code} | {desc}" for desc, code in existing_codes.items()]
        processing_method = "chatgpt"
        
        # Usa APENAS ChatGPT - sem fallback local
        try:
            # group_with_chatgpt retorna (codes_dict, groups_dict) ou ({}, {}) em caso de erro
            # Passamos apenas os itens √öNICOS para criar o Codebook
            codes_ret, groups_ret = self.coding_system.group_with_chatgpt(unique_items, f17=f17_list)
            
            # --- L√ìGICA DE RETRY PARA ITENS N√ÉO MAPEADOS ---
            # Verifica quais itens √∫nicos N√ÉO foram cobertos por nenhum grupo retornado nem pelo F17
            
            # Cria conjunto de tudo que foi coberto
            covered_items = set()
            # 1. Coberto pelo retorno do GPT
            for respostas_grupo in groups_ret.values():
                for r in respostas_grupo:
                    covered_items.add(str(r).strip())
                    # Normaliza√ß√£o agressiva
                    covered_items.add(self.coding_system.normalize_text(str(r)))
            
            # 2. Coberto pelo F17 existente (backup local)
            # Normaliza chaves do F17 para garantir match
            normalized_f17 = {self.coding_system.normalize_text(k): v for k, v in existing_codes.items()}
            
            items_missing = []
            for item in unique_items:
                item_str = str(item).strip()
                item_norm = self.coding_system.normalize_text(item_str)
                
                # Verifica cobertura
                is_covered = False
                
                # Checa match exato ou normalizado nos grupos do GPT
                if item_str in covered_items or item_norm in covered_items:
                    is_covered = True
                
                # Checa match no F17 (exato ou normalizado)
                if not is_covered:
                    if item_str in existing_codes:
                        is_covered = True
                    elif item_norm in normalized_f17:
                        is_covered = True
                
                # Se n√£o coberto, adiciona para retry
                if not is_covered:
                    items_missing.append(item_str)
            
            if items_missing:
                print(f"[DEBUG] ‚ö†Ô∏è {len(items_missing)} itens n√£o foram processados na primeira passada. Tentando RETRY...", flush=True)
                print(f"[DEBUG] Itens faltando para Retry: {items_missing}", flush=True)
                
                # Segunda chamada APENAS para os faltantes
                try:
                    # Prompt espec√≠fico para o retry - FOR√áA a cria√ß√£o de c√≥digos
                    retry_prompt_suffix = "\n\nIMPORTANT: You MUST provide a code for EACH of these remaining items. Do not skip any."
                    
                    # Usa uma chamada dedicada para o retry se poss√≠vel, ou a mesma fun√ß√£o
                    # Aqui usamos a mesma fun√ß√£o mas passando apenas os faltantes
                    codes_retry, groups_retry = self.coding_system.group_with_chatgpt(items_missing, f17=f17_list)
                    
                    if codes_retry and groups_retry:
                        print(f"[DEBUG] ‚úÖ Retry bem sucedido! Recuperados {len(codes_retry)} novos c√≥digos.", flush=True)
                        
                        # Calcula o pr√≥ximo c√≥digo dispon√≠vel com seguran√ßa
                        used_codes = set(codes_ret.values()) | set(existing_codes.values())
                        max_code = max(used_codes) if used_codes else 9
                        
                        for titulo, respostas in groups_retry.items():
                            # Se o t√≠tulo j√° existe, mescla respostas
                            if titulo in groups_ret:
                                for r in respostas:
                                    if r not in groups_ret[titulo]:
                                        groups_ret[titulo].append(r)
                            else:
                                # Novo c√≥digo
                                novo_cod = codes_retry[titulo]
                                # Se conflitar, incrementa
                                if novo_cod in used_codes:
                                    max_code += 1
                                    novo_cod = max_code
                                    used_codes.add(novo_cod)
                                
                                codes_ret[titulo] = novo_cod
                                groups_ret[titulo] = respostas
                    else:
                         print(f"[DEBUG] ‚ùå Retry retornou vazio.", flush=True)

                except Exception as e_retry:
                    print(f"[DEBUG] Erro no retry: {e_retry}", flush=True)
                
                # --- FALLBACK FINAL (√öltima Milha) ---
                # Se ainda sobraram itens sem c√≥digo ap√≥s o retry, cria c√≥digos novos automaticamente
                # Isso garante que NUNCA retornamos 'ERROR' para itens v√°lidos
                
                # Recalcula o que ainda falta
                still_missing = []
                covered_now = set()
                for resps in groups_ret.values():
                    for r in resps:
                        covered_now.add(str(r).strip())
                        covered_now.add(self.coding_system.normalize_text(str(r)))
                
                # Atualiza com F17
                normalized_f17 = {self.coding_system.normalize_text(k): v for k, v in existing_codes.items()}
                
                for item in items_missing:
                    item_str = str(item).strip()
                    item_norm = self.coding_system.normalize_text(item_str)
                    if item_str not in covered_now and item_norm not in covered_now:
                        if item_str not in existing_codes and item_norm not in normalized_f17:
                            still_missing.append(item_str)
                
                if still_missing:
                    print(f"[DEBUG] ‚ö†Ô∏è Ainda restam {len(still_missing)} itens ap√≥s retry. Criando c√≥digos autom√°ticos...", flush=True)
                    
                    # Determina pr√≥ximo c√≥digo seguro
                    used_codes = set(codes_ret.values()) | set(existing_codes.values())
                    next_code = (max(used_codes) + 1) if used_codes else 10
                    if next_code < 10: next_code = 10 # Garante m√≠nimo 10 para novos
                    
                    for missing_item in still_missing:
                        # Cria um grupo novo para cada item faltante
                        # Usa o pr√≥prio texto como t√≠tulo (capitalizado)
                        title = missing_item.capitalize()
                        
                        # Verifica se j√° existe grupo com esse t√≠tulo (improv√°vel mas poss√≠vel)
                        if title in groups_ret:
                            groups_ret[title].append(missing_item)
                        else:
                            while next_code in used_codes:
                                next_code += 1
                            
                            codes_ret[title] = next_code
                            groups_ret[title] = [missing_item]
                            used_codes.add(next_code)
                            
                    print(f"[DEBUG] ‚úÖ {len(still_missing)} c√≥digos autom√°ticos criados.", flush=True)

            else:
                 print(f"[DEBUG] ‚úÖ Todos os itens √∫nicos foram cobertos na primeira passada (ou est√£o no F17).", flush=True)

            # -----------------------------------------------

        except Exception as e_gpt:
            # Propaga a exce√ß√£o diretamente - a mensagem j√° foi formatada no group_with_chatgpt
            # N√£o adiciona mais contexto para evitar duplica√ß√£o
            raise e_gpt

        if not codes_ret or not groups_ret:
            error_msg = "ChatGPT retornou resultado vazio ou inv√°lido. Verifique sua chave de API e tente novamente."
            print(f"[DEBUG] {error_msg}", flush=True)
            raise Exception(error_msg)
        
        # Usa diretamente o retorno do ChatGPT (mapeando para existing_codes quando aplic√°vel)
        codes = codes_ret.copy()
        groups = groups_ret.copy()
        
        # GARANTIA DE F17: Adiciona c√≥digos existentes ao dicion√°rio de c√≥digos se n√£o estiverem l√°
        # Isso garante que respostas que o GPT ignorou (por j√° existirem) sejam encontradas
        for desc, code in existing_codes.items():
             # Normaliza a chave do F17 para garantir match
             desc_norm = self.coding_system.correct_text(desc).strip()
             if desc_norm not in codes:
                 codes[desc_norm] = code
                 # Cria grupo fict√≠cio se n√£o existir, para o relat√≥rio
                 if desc_norm not in groups:
                     groups[desc_norm] = []

        # se alguma descri√ß√£o do ChatGPT corresponder ao F17, ajuste para usar a descri√ß√£o do F17
        normalized_map = {self.coding_system.correct_text(desc).strip().lower(): desc for desc in existing_codes.keys()}
        
        # Mapa reverso de c√≥digos F17 para evitar conflitos
        # code -> description
        existing_codes_reverse = {v: k for k, v in existing_codes.items()}
        
        # Encontra o maior c√≥digo em uso para gerar novos
        if existing_codes:
            max_existing_code = max(existing_codes.values())
        else:
            max_existing_code = 9
            
        adjusted_codes = {}
        adjusted_groups = {}
        
        # Processa retorno do ChatGPT validando conflitos
        for titulo, respostas in groups.items():
            titulo_norm = self.coding_system.correct_text(titulo).strip().lower()
            
            # Caso 1: T√≠tulo bate com F17 (ex: "Tubar√£o") -> Usa c√≥digo F17
            if titulo_norm in normalized_map:
                f17_desc = normalized_map[titulo_norm]
                code_to_use = existing_codes[f17_desc]
                
                adjusted_codes[f17_desc] = code_to_use
                adjusted_groups[f17_desc] = respostas
                
            else:
                # Caso 2: T√≠tulo Novo (ex: "Chapec√≥")
                proposed_code = codes.get(titulo)
                
                # Verifica se o c√≥digo proposto j√° existe no F17 para OUTRA coisa
                if proposed_code in existing_codes_reverse:
                    # Conflito! O c√≥digo 1 √© "Tubar√£o", mas GPT quer usar para "Chapec√≥"
                    # Gera novo c√≥digo
                    max_existing_code += 1
                    final_code = max_existing_code
                else:
                    # C√≥digo livre ou √© novo
                    # Mas cuidado: se GPT retornou 1 e n√£o existe no F17, ok. 
                    # Mas se retornou 10, ok.
                    # Melhor garantir que seja > max do F17 se for novo
                    if proposed_code is not None and proposed_code <= max(existing_codes.values() or [0]) and proposed_code not in existing_codes.values():
                         # GPT inventou um c√≥digo baixo livre? Aceita.
                         final_code = proposed_code
                    elif proposed_code is not None and proposed_code > max(existing_codes.values() or [0]):
                         final_code = proposed_code
                         if final_code > max_existing_code: max_existing_code = final_code
                    else:
                         # Sem c√≥digo ou conflito, gera novo
                         max_existing_code += 1
                         final_code = max_existing_code
                
                adjusted_codes[titulo] = final_code
                adjusted_groups[titulo] = respostas
        
        # Re-adiciona F17 que podem ter sido perdidos no loop acima se n√£o estavam em 'groups'
        for desc, code in existing_codes.items():
             if desc not in adjusted_codes:
                 adjusted_codes[desc] = code
                 adjusted_groups[desc] = []

        codes = adjusted_codes
        groups = adjusted_groups
        detailed_report = self.coding_system.create_detailed_report(codes, groups, question_name, processing_method)
        print(f"[DEBUG] ‚úÖ Processamento conclu√≠do usando: CHATGPT (OpenAI)", flush=True)
        
        # CONSOLIDA√á√ÉO FINAL DOS C√ìDIGOS
        # Garante que 'codes' (usado para o loop de classifica√ß√£o) e 'final_codes' (retorno)
        # contenham a uni√£o de tudo: F17 + GPT + Retry + Auto
        
        # Adiciona F17
        for desc, code in existing_codes.items():
            if desc not in codes:
                codes[desc] = code
        
        # Garante que novos c√≥digos do Auto/Retry estejam em 'codes'
        # (j√° foram adicionados a codes_ret que √© a base de codes, mas refor√ßando)
        
        print(f"[DEBUG] C√≥digos finais consolidados: {len(codes)} itens", flush=True)
        # print(f"[DEBUG] Amostra de c√≥digos: {list(codes.items())[:5]}", flush=True)

        # Cria mapeamento de respostas para c√≥digos (com normaliza√ß√£o)
        response_to_code = {}
        total_respostas_mapeadas = 0
        
        # Reconstr√≥i response_to_code com base no dicion√°rio consolidado 'codes' e 'groups'
        # Se um c√≥digo est√° em 'codes' mas n√£o em 'groups', cria entrada dummy no loop
        
        all_keys = set(groups.keys()) | set(codes.keys())
        
        for titulo in all_keys:
            code = codes.get(titulo)
            respostas = groups.get(titulo, [])
            
            if code:
                # Adiciona o pr√≥prio t√≠tulo do grupo como chave de busca
                titulo_str = str(titulo).strip()
                titulo_norm = self.coding_system.normalize_text(titulo_str)
                response_to_code[titulo_str] = (code, titulo_str)
                response_to_code[titulo_norm] = (code, titulo_str)
                
                for resposta in respostas:
                    resp_norm = self.coding_system.normalize_text(str(resposta))
                    response_to_code[str(resposta).strip()] = (code, resposta)
                    response_to_code[resp_norm] = (code, resposta)
                    total_respostas_mapeadas += 1
                    
        print(f"[DEBUG] Mapeamento criado: {len(all_keys)} grupos, {total_respostas_mapeadas} respostas mapeadas, {len(response_to_code)} chaves no dicion√°rio", flush=True)
        print(f"[DEBUG] Total de respostas originais para processar: {len(question_data)}", flush=True)
        
        code_column = []
        response_column = []
        
        # Se for semi-aberta, precisamos reconstruir a coluna misturando os originais num√©ricos com os codificados
        if q_type == "semi-aberta":
             # Inicializa com os valores originais (assumindo que s√£o c√≥digos)
            code_column = [x for x in question_data]
            response_column = [x for x in question_data]
            
            # Processa apenas os itens que foram para a IA
            # Mapeamento inverso dos grupos para encontrar c√≥digos
            text_to_code = {}
            # Normaliza chaves do groups_ret/codes_ret para busca
            for titulo, respostas in groups.items():
                 c = codes.get(titulo)
                 if c:
                     for r in respostas:
                         text_to_code[self.coding_system.normalize_text(str(r))] = c
                         text_to_code[str(r).strip()] = c # Original tamb√©m
            
            # Atualiza apenas os √≠ndices que eram texto
            for idx, original_resp in zip(indices_to_process, items_to_process):
                # Tenta encontrar o c√≥digo
                resp_norm = self.coding_system.normalize_text(str(original_resp))
                found_code = text_to_code.get(resp_norm) or text_to_code.get(str(original_resp).strip())
                
                # Tenta fuzzy se n√£o achou (reuso da l√≥gica abaixo seria ideal, mas simplificando aqui)
                if not found_code:
                     # Tenta achar no response_to_code criado anteriormente (se existir) ou busca direta
                     # Por hora, marca ERROR se n√£o achar, ou usa a l√≥gica de match completa
                     pass

                # ATEN√á√ÉO: A l√≥gica abaixo de loop principal j√° faz o match robusto.
                # Vamos deixar o loop principal rodar APENAS para os items_to_process se for semi-aberta?
                # N√£o, o loop principal espera iterar sobre 'question_data'.
                # Vamos ajustar o loop principal para lidar com o tipo de quest√£o.
                pass 

        # REFAZENDO O LOOP PRINCIPAL PARA SUPORTAR OS TIPOS
        code_column = []
        response_column = []
        
        for i, resp in enumerate(question_data):
            # Se for semi-aberta e este √≠ndice N√ÉO foi processado (era n√∫mero), mant√©m original
            if q_type == "semi-aberta" and i not in indices_to_process:
                code_column.append(resp) # Mant√©m o n√∫mero original
                response_column.append(resp)
                continue
                
            found = False
            
            # Se for c√≥digo num√©rico expl√≠cito (NS/NR) em quest√£o aberta, mant√©m
            if isinstance(resp, (int, float)):
                resp_num = int(resp)
                if resp_num in [55, 66, 77, 88, 98, 99]:
                    code_column.append(resp_num)
                    response_column.append(resp)
                    found = True
            
            if not found:
                # Normaliza resposta original para busca
                resp_str = str(resp).strip()
                resp_norm = self.coding_system.normalize_text(resp_str)
                
                # Tenta match exato (sem normaliza√ß√£o primeiro)
                if resp_str in response_to_code:
                    code, resp_original = response_to_code[resp_str]
                    code_column.append(code)
                    response_column.append(resp)
                    found = True
                # Tenta match exato normalizado
                elif resp_norm in response_to_code:
                    code, resp_original = response_to_code[resp_norm]
                    code_column.append(code)
                    response_column.append(resp)
                    found = True
                else:
                    # Tenta match parcial/fuzzy
                    best_match = None
                    best_score = 0
                    for resp_norm_key, (code, resp_original) in response_to_code.items():
                        # Match exato ap√≥s normaliza√ß√£o
                        if resp_norm == resp_norm_key:
                            best_match = (code, resp)
                            best_score = 100
                            break
                        # Match parcial (substring)
                        if resp_norm in resp_norm_key or resp_norm_key in resp_norm:
                            score = min(len(resp_norm), len(resp_norm_key)) / max(len(resp_norm), len(resp_norm_key), 1)
                            if score > best_score:
                                best_score = score
                                best_match = (code, resp)
                    
                    if best_match and best_score >= 0.8:  # Threshold de 80%
                        code, resp_original = best_match
                        code_column.append(code)
                        response_column.append(resp)
                        found = True
                    else:
                        # Tenta fuzzy matching como √∫ltimo recurso
                        from fuzzywuzzy import fuzz
                        for resp_norm_key, (code, resp_original) in response_to_code.items():
                            score = fuzz.ratio(resp_norm, resp_norm_key)
                            if score > best_score:
                                best_score = score
                                best_match = (code, resp)
                        
                        if best_match and best_score >= 85:  # Threshold de 85% para fuzzy
                            code, resp_original = best_match
                            code_column.append(code)
                            response_column.append(resp)
                            found = True
            
            if not found:
                # N√£o encontrou match - marca como ERROR
                resp_debug = str(resp).strip()
                resp_norm_debug = self.coding_system.normalize_text(resp_debug)
                print(f"[DEBUG] Resposta n√£o mapeada: '{resp_debug}' (normalizada: '{resp_norm_debug}')", flush=True)
                if len(response_to_code) > 0:
                     # Debug limitado
                     pass
                code_column.append('ERROR')
                response_column.append(resp)
        new_codes = {desc: code for desc, code in codes.items() if desc not in existing_codes}
        print(f"[DEBUG] Sa√≠da de process_single_question_with_chatgpt: codes={codes}, new_codes={new_codes}", flush=True)
        
        # RECONSTRU√á√ÉO DO DICION√ÅRIO 'GROUPS' PARA RELAT√ìRIO COMPLETO
        # O 'groups' atual tem apenas a amostra do GPT. Precisamos de TODAS as respostas originais agrupadas
        # para que o relat√≥rio mostre todas as varia√ß√µes (ex: Tubara, Tubarao, Tubar√£o)
        
        full_groups = {}
        # Inicializa com as chaves conhecidas
        for desc in codes.keys():
            full_groups[desc] = []
            
        # Itera sobre o resultado classificado (resposta -> c√≥digo)
        # Precisa do mapeamento reverso c√≥digo -> descri√ß√£o
        code_to_desc = {v: k for k, v in codes.items()}
        
        for resp, code in zip(response_column, code_column):
            if code != 'ERROR' and code in code_to_desc:
                desc = code_to_desc[code]
                # Adiciona TODAS as respostas para auditoria completa (Op√ß√£o B)
                # Se preferir lista limpa, descomente o if abaixo
                # if resp not in full_groups[desc]:
                full_groups[desc].append(resp)
        
        # Atualiza o objeto groups para o relat√≥rio
        groups = full_groups
        
        # Regera o relat√≥rio com os dados completos
        detailed_report = self.coding_system.create_detailed_report(codes, groups, question_name, processing_method)
        
        return {
            'question_name': question_name,
            'total_responses': len(question_data),
            'valid_responses': len(question_data),
            'existing_codes': existing_codes,
            'final_codes': codes,
            'new_codes': new_codes,
            'groups': groups, # Agora cont√©m TODAS as respostas originais agrupadas
            'detailed_report': detailed_report,
            'code_column': code_column,
            'response_column': response_column,
            'processing_method': processing_method or 'desconhecido',
            'question_type': q_type,
            'statistics': {
                'total_codes': len(codes),
                'new_codes_count': len(new_codes),
                'groups_with_multiple': len([g for g in groups.values() if len(g) > 1]),
                'largest_group_size': max([len(g) for g in groups.values()]) if groups else 0
            }
        }
    
    def save_improved_outputs(self, result: Dict[str, Any], output_dir: str = ".") -> Dict[str, str]:
        """Salva arquivos de sa√≠da melhorados"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_name = result['question_name'].replace('/', '_').replace('?', '').replace(':', '')[:30]
        base_name = f"{safe_name}_{timestamp}"
        
        files_created = {}
        
        # 1. Banco codificado
        banco_path = os.path.join(output_dir, f"{base_name}_banco_codificado.xlsx")
        banco_df = pd.DataFrame({
            'C√≥digo': result['code_column'],
            'Resposta': result['response_column']
        })
        banco_df.to_excel(banco_path, index=False)
        files_created['banco'] = banco_path
        
        # 2. F17 atualizado
        f17_path = os.path.join(output_dir, f"{base_name}_f17_atualizado.xlsx")
        
        # Padroniza descri√ß√µes antes de salvar
        # Separa o que √© do F17 original (confi√°vel) do que √© novo (precisa de revis√£o)
        original_f17_descs = set(result['existing_codes'].keys())
        
        final_f17_list = []
        for desc, code in sorted(result['final_codes'].items(), key=lambda x: x[1]):
            final_desc = desc
            # Se √© um c√≥digo novo (n√£o estava no F17 original), tenta corrigir a grafia
            if desc not in original_f17_descs:
                # Usa o corretor do sistema (que pode usar GPT se dispon√≠vel ou local)
                # A fun√ß√£o standardize_with_chatgpt j√° verifica disponibilidade
                try:
                    # Remove aspas extras se houver
                    clean_desc = desc.strip('"').strip("'")
                    # Tenta padronizar via GPT ou corretor local
                    if hasattr(self.coding_system, 'standardize_with_chatgpt') and getattr(self.coding_system, 'chatgpt_available', False):
                         final_desc = self.coding_system.standardize_with_chatgpt(clean_desc)
                    else:
                         final_desc = self.coding_system.correct_text(clean_desc)
                except Exception:
                    final_desc = desc # Mant√©m original em caso de erro
            
            final_f17_list.append({'C√≥digo': code, 'Descri√ß√£o': final_desc})

        f17_df = pd.DataFrame(final_f17_list)
        f17_df.to_excel(f17_path, index=False)
        files_created['f17'] = f17_path
        
        # 3. Relat√≥rio detalhado de agrupamentos
        relatorio_path = os.path.join(output_dir, f"{base_name}_relatorio_agrupamentos.txt")
        with open(relatorio_path, 'w', encoding='utf-8') as f:
            f.write(result['detailed_report'])
        files_created['relatorio'] = relatorio_path
        
        # 4. Resumo estat√≠stico
        resumo_path = os.path.join(output_dir, f"{base_name}_resumo_estatistico.txt")
        resumo_content = self.create_statistical_summary(result)
        with open(resumo_path, 'w', encoding='utf-8') as f:
            f.write(resumo_content)
        files_created['resumo'] = resumo_path
        
        return files_created
    
    def create_statistical_summary(self, result: Dict[str, Any]) -> str:
        """Cria resumo estat√≠stico do processamento"""
        
        lines = []
        lines.append(f"RESUMO ESTAT√çSTICO - {result['question_name']}")
        lines.append("=" * 60)
        lines.append(f"Data: {datetime.now().strftime('%d/%m/%Y %H:%M')}")
        lines.append("")
        
        # M√©todo de processamento
        processing_method = result.get('processing_method', 'desconhecido')
        method_display = {
            'chatgpt': 'ü§ñ ChatGPT (OpenAI GPT-4o)',
            'fallback_local': 'üîß Agrupador Local (Fallback)',
            'desconhecido': '‚ùì M√©todo Desconhecido'
        }
        lines.append("M√âTODO DE PROCESSAMENTO:")
        lines.append(f"- {method_display.get(processing_method, f'‚ùì {processing_method}')}")
        lines.append("")
        
        # Calcula frequ√™ncias REAIS baseadas na coluna de c√≥digos final
        # (result['groups'] tem apenas exemplos √∫nicos, n√£o serve para contagem estat√≠stica)
        code_counts = {}
        for code in result['code_column']:
            if code != 'ERROR':
                code_counts[code] = code_counts.get(code, 0) + 1
                
        # Estat√≠sticas gerais
        lines.append("ESTAT√çSTICAS GERAIS:")
        lines.append(f"- Total de respostas: {result['total_responses']}")
        lines.append(f"- Respostas v√°lidas: {result['valid_responses']}")
        lines.append(f"- C√≥digos existentes (F17): {len(result['existing_codes'])}")
        lines.append(f"- Novos c√≥digos criados: {result['statistics']['new_codes_count']}")
        lines.append(f"- Total de c√≥digos finais: {result['statistics']['total_codes']}")
        lines.append("")
        
        # An√°lise de agrupamentos
        # Recalcula m√©tricas com base nas frequ√™ncias reais
        groups_with_multiple = len([c for c, count in code_counts.items() if count > 1])
        largest_group_size = max(code_counts.values()) if code_counts else 0
        
        lines.append("AN√ÅLISE DE AGRUPAMENTOS:")
        lines.append(f"- Grupos com m√∫ltiplas respostas: {groups_with_multiple}")
        lines.append(f"- Maior grupo: {largest_group_size} respostas")
        lines.append("")
        
        # C√≥digos existentes utilizados
        existing_used = []
        for desc, code in result['existing_codes'].items():
            count = code_counts.get(code, 0)
            if count > 0:
                existing_used.append((desc, code, count))
        
        if existing_used:
            lines.append("C√ìDIGOS EXISTENTES UTILIZADOS:")
            for desc, code, size in sorted(existing_used, key=lambda x: x[1]):
                lines.append(f"- C√≥digo {code}: {desc} ({size} respostas)")
            lines.append("")
        
        # Novos c√≥digos criados
        if result['new_codes']:
            lines.append("NOVOS C√ìDIGOS CRIADOS:")
            for desc, code in sorted(result['new_codes'].items(), key=lambda x: x[1]):
                count = code_counts.get(code, 0)
                lines.append(f"- C√≥digo {code}: {desc} ({count} respostas)")
            lines.append("")
        
        # Grupos com m√∫ltiplas respostas (Top 10)
        # Usa a contagem real para ordenar
        multi_groups = []
        for desc, code in result['final_codes'].items():
            count = code_counts.get(code, 0)
            if count > 1:
                # Pega exemplos do grupo para exibi√ß√£o
                examples = result['groups'].get(desc, [])
                multi_groups.append((desc, code, count, examples))
                
        if multi_groups:
            lines.append("PRINCIPAIS AGRUPAMENTOS:")
            for desc, code, count, examples in sorted(multi_groups, key=lambda x: x[2], reverse=True)[:10]:
                lines.append(f"- C√≥digo {code} ({count} respostas): {desc}")
                for resp in examples[:3]:
                    lines.append(f"  ‚Ä¢ {resp}")
                if len(examples) > 3:
                    lines.append(f"  ‚Ä¢ ... e mais {len(examples) - 3} varia√ß√µes")
                lines.append("")
        
        return "\n".join(lines)

