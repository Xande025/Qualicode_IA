"""
Sistema de Codifica√ß√£o IPO Melhorado
- Corre√ß√£o ortogr√°fica antes do agrupamento
- An√°lise de sentido para agrupamento inteligente
- Relat√≥rio detalhado mostrando todos os agrupamentos
"""

import pandas as pd
import re
from typing import Dict, List, Tuple, Any
from collections import defaultdict
import os
import json
from datetime import datetime
from openai import OpenAI

def get_openai_client(api_key):
    return OpenAI(api_key=api_key)
from dotenv import load_dotenv
from fuzzywuzzy import fuzz
import unicodedata
from dotenv import load_dotenv
from fuzzywuzzy import fuzz
import unicodedata
from cache_manager import CacheManager
load_dotenv()

class ImprovedIPOCodingSystem:
    """Sistema de codifica√ß√£o melhorado com relat√≥rio detalhado"""
    
    def __init__(self):
        self.corrections = self.load_corrections()
        self.similarity_patterns = self.load_similarity_patterns()
        # Flag indicando se a API do ChatGPT est√° dispon√≠vel (True/False/None)
        # None = n√£o testado ainda, True = dispon√≠vel, False = indispon√≠vel
        # Flag indicando se a API do ChatGPT est√° dispon√≠vel (True/False/None)
        # None = n√£o testado ainda, True = dispon√≠vel, False = indispon√≠vel
        self.chatgpt_available = None
        self.cache = CacheManager()
    
    def load_corrections(self) -> Dict[str, str]:
        """Carrega corre√ß√µes ortogr√°ficas de arquivo JSON ou usa padr√£o"""
        default_corrections = {
            'nao': 'n√£o',
            'sao': 's√£o',
            'voce': 'voc√™',
            'esta': 'est√°',
            'saude': 'sa√∫de',
            'educacao': 'educa√ß√£o',
            'administracao': 'administra√ß√£o',
            'pavimentacao': 'pavimenta√ß√£o',
            'iluminacao': 'ilumina√ß√£o',
            'seguranca': 'seguran√ßa',
            'transito': 'tr√¢nsito',
            'prefeitura': 'prefeitura',
            'prefeito': 'prefeito',
            'otimo': '√≥timo',
            'pessimo': 'p√©ssimo',
            'muito': 'muito',
            'tambem': 'tamb√©m',
            'melhor': 'melhor',
            'pior': 'pior',
            'enchente': 'enchente',
            'enchentes': 'enchentes',
            'enche tez': 'enchentes',
            'asfalto': 'asfalto',
            'asfaltamento': 'asfaltamento',
            'calcamento': 'cal√ßamento',
            'calcadas': 'cal√ßadas',
            'cemai': 'Cemai',
            'semae': 'Semae',
            'upa': 'UPA'
        }
        
        try:
            # Tenta carregar do arquivo config/corrections.json
            base_dir = os.path.dirname(os.path.abspath(__file__))
            config_path = os.path.join(base_dir, 'config', 'corrections.json')
            
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"Erro ao carregar corre√ß√µes do arquivo: {e}. Usando padr√£o.")
            
        return default_corrections
    
    def load_similarity_patterns(self) -> Dict[str, List[str]]:
        """Carrega padr√µes de similaridade de arquivo JSON ou usa padr√£o"""
        default_patterns = {
            'saude': ['sa√∫de', 'posto', 'm√©dico', 'hospital', 'atendimento m√©dico', 'consulta'],
            'asfalto': ['asfalto', 'pavimenta√ß√£o', 'pavimentar', 'rua', 'estrada', 'asfaltamento'],
            'educacao': ['educa√ß√£o', 'escola', 'ensino', 'curso', 'qualifica√ß√£o', 'instituto'],
            'enchente': ['enchente', 'enchentes', 'alagamento', '√°gua', 'inunda√ß√£o'],
            'habitacao': ['casa', 'moradia', 'habita√ß√£o', 'lote', 'apartamento', 'albergue'],
            'infraestrutura': ['cal√ßada', 'cal√ßamento', 'ponte', 'obra', 'constru√ß√£o'],
            'esporte': ['esporte', 'projeto', 'lazer', 'recrea√ß√£o'],
            'empresa': ['empresa', 'f√°brica', 'emprego', 'trabalho', 'desenvolvimento'],
            'nada': ['nada', 'nenhum', 'n√£o fez', 'n√£o tem', 'ruim']
        }
        
        try:
            # Tenta carregar do arquivo config/similarity_patterns.json
            base_dir = os.path.dirname(os.path.abspath(__file__))
            config_path = os.path.join(base_dir, 'config', 'similarity_patterns.json')
            
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"Erro ao carregar padr√µes de similaridade do arquivo: {e}. Usando padr√£o.")
            
        return default_patterns
    
    def correct_text(self, text: str) -> str:
        """Corrige ortografia do texto"""
        if pd.isna(text) or not isinstance(text, str):
            return str(text)
        
        corrected = str(text).strip().lower()
        
        # Aplica corre√ß√µes palavra por palavra
        words = corrected.split()
        corrected_words = []
        
        for word in words:
            # Remove pontua√ß√£o para compara√ß√£o
            clean_word = re.sub(r'[^\w]', '', word)
            if clean_word in self.corrections:
                # Substitui mantendo pontua√ß√£o original
                corrected_word = word.replace(clean_word, self.corrections[clean_word])
                corrected_words.append(corrected_word)
            else:
                corrected_words.append(word)
        
        corrected = ' '.join(corrected_words)
        
        # Capitaliza primeira letra
        if corrected:
            corrected = corrected[0].upper() + corrected[1:] if len(corrected) > 1 else corrected.upper()
        
        # Remove espa√ßos duplos
        corrected = re.sub(r'\s+', ' ', corrected).strip()
        
        return corrected

    def normalize_text(self, text: str) -> str:
        """Normaliza texto para compara√ß√µes: remove acentos, pontua√ß√£o, lowercase e espa√ßos extras."""
        if text is None:
            return ''
        s = str(text).strip().lower()
        # remove acentos
        s = unicodedata.normalize('NFKD', s)
        s = ''.join([c for c in s if not unicodedata.combining(c)])
        # remove pontua√ß√£o
        s = re.sub(r'[^a-z0-9\s]', ' ', s)
        s = re.sub(r'\s+', ' ', s).strip()
        return s

    def canonicalize(self, text: str) -> str:
        """Produz forma can√¥nica para agrupar respostas equivalentes."""
        if not text:
            return ''
        # aplica corre√ß√µes ortogr√°ficas primeiro
        corrected = self.correct_text(text)
        norm = self.normalize_text(corrected)

        # regras simples de sin√¥nimos/normaliza√ß√£o
        syn_map = {
            'onibus': 'onibus',
            '√¥nibus': 'onibus',
            'onibis': 'onibus',
            'asfaltmento': 'asfalto',
            'asfalto': 'asfalto',
            'pavimentacao': 'asfalto',
            'pavimentacao/asfalto': 'asfalto',
            'posto de saude': 'posto saude',
            'posto medico': 'posto saude',
            'muito medico': 'medico',
            'mais medicos': 'medico',
            'medicos no posto': 'medico',
            'polisia': 'policia',
            'policia': 'policia',
            'policiamento nas ruas': 'policiamento',
            'policiamento melhor': 'policiamento',
            'seguranca nas ruas': 'seguranca',
            'seguranca publica': 'seguranca',
            'mais seguranca': 'seguranca',
            'egotos': 'esgoto',
            'esgotos emtupidos': 'esgoto',
            'saneamento basico': 'saneamento',
        }

        if norm in syn_map:
            return syn_map[norm]

        # tenta mapear por palavras-chave conhecidas
        for key, keywords in self.similarity_patterns.items():
            for kw in keywords:
                if kw in norm:
                    return key

        return norm
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """Calcula similaridade entre dois textos"""
        if not text1 or not text2:
            return 0.0
        
        text1_lower = text1.lower()
        text2_lower = text2.lower()
        
        # Similaridade exata
        if text1_lower == text2_lower:
            return 1.0
        
        # Similaridade por palavras-chave
        similarity_score = 0.0
        
        for category, keywords in self.similarity_patterns.items():
            text1_matches = sum(1 for keyword in keywords if keyword in text1_lower)
            text2_matches = sum(1 for keyword in keywords if keyword in text2_lower)
            
            if text1_matches > 0 and text2_matches > 0:
                similarity_score += 0.8  # Alta similaridade por categoria
        
        # Similaridade por palavras comuns
        words1 = set(text1_lower.split())
        words2 = set(text2_lower.split())
        
        if words1 and words2:
            common_words = words1.intersection(words2)
            word_similarity = len(common_words) / max(len(words1), len(words2))
            similarity_score += word_similarity * 0.5
        
        return min(similarity_score, 1.0)
    
    def group_responses_intelligent(self, responses: List[str], existing_codes: Dict[str, int], 
                                  similarity_threshold: float = 0.6) -> Tuple[Dict[str, int], Dict[str, List[str]]]:
        """Agrupa respostas de forma inteligente"""
        # Normaliza e corrige respostas e existing_codes
        corrected_responses = []
        for response in responses:
            if pd.isna(response) or not str(response).strip():
                corrected_responses.append(response)
            else:
                corrected_responses.append(self.correct_text(str(response)))

        # Normaliza existing_codes para compara√ß√£o (mant√©m mapa para recuperar descri√ß√£o original)
        norm_existing_map = {}  # norm_desc -> (original_desc, code)
        for desc, code in existing_codes.items():
            norm = self.correct_text(str(desc)).strip().lower()
            norm_existing_map[norm] = (desc, code)

        # Inicializa grupos com chaves j√° existentes (usando a descri√ß√£o original)
        groups = defaultdict(list)
        for desc, code in existing_codes.items():
            groups[desc] = []

        # Pr√≥ximo c√≥digo dispon√≠vel (ignora reservados)
        reserved_codes = [55, 66, 77, 88, 99]
        used_codes = [c for c in existing_codes.values() if c not in reserved_codes]
        next_code = max(used_codes) + 1 if used_codes else 10

        # Processa cada resposta, priorizando c√≥digos do F17
        processed = set()
        for original_response, corrected_response in zip(responses, corrected_responses):
            if pd.isna(original_response) or original_response in processed:
                continue

            resp_str = str(original_response).strip()

            # Se a resposta for n√∫mero e existir como c√≥digo, agrupa corretamente
            try:
                resp_num = int(resp_str)
                if resp_num in existing_codes.values():
                    for desc, code in existing_codes.items():
                        if code == resp_num:
                            groups[desc].append(original_response)
                            processed.add(original_response)
                            break
                    continue
            except Exception:
                pass

            # Normaliza e canonicaliza para compara√ß√£o
            corr_norm = self.canonicalize(corrected_response) if corrected_response is not None else ''

            # 1) Tenta match exato com existing_codes normalizado
            matched = False
            if corr_norm in norm_existing_map:
                desc, code = norm_existing_map[corr_norm]
                groups[desc].append(original_response)
                processed.add(original_response)
                matched = True
                continue

            # 2) Tenta match fuzzy com existing_codes
            best_desc = None
            best_score = 0
            for norm_desc, (orig_desc, code) in norm_existing_map.items():
                score = fuzz.token_set_ratio(corr_norm, norm_desc)
                if score > best_score:
                    best_score = score
                    best_desc = orig_desc
            if best_score >= 85:
                groups[best_desc].append(original_response)
                processed.add(original_response)
                continue

            # 3) Procura grupo novo j√° criado (usa canonical forms + fuzzy)
            best_match = None
            best_score = 0
            for group_key in list(groups.keys()):
                # ignora grupos que s√£o F17 existentes (j√° tratados acima)
                if group_key in existing_codes:
                    continue
                key_norm = self.canonicalize(group_key)
                # verifica√ß√£o direta de igualdade can√¥nica
                if key_norm and corr_norm and key_norm == corr_norm:
                    best_match = group_key
                    best_score = 100
                    break
                # fuzzy nas formas normalizadas
                sim = fuzz.token_set_ratio(corr_norm, key_norm)
                if sim > best_score:
                    best_score = sim
                    best_match = group_key
            if best_score >= 82:
                groups[best_match].append(original_response)
                processed.add(original_response)
                continue

            # 4) Cria novo grupo com a forma corrigida padronizada
            # Cria novo grupo usando forma can√¥nica por√©m apresent√°vel (capitalizada)
            canonical = self.canonicalize(corrected_response) if corrected_response is not None else ''
            display_key = self.correct_text(canonical) if canonical else self.correct_text(str(original_response))
            new_key = display_key
            groups[new_key].append(original_response)
            processed.add(original_response)

        # Mescla grupos similares (usa fuzzy) e reconstr√≥i o mapa de c√≥digos garantindo n√£o duplicar
        merged = self.merge_similar_groups(dict(groups), threshold=85)

        # Reconstr√≥i c√≥digos: prioriza existing_codes; novos come√ßam em next_code
        codes = existing_codes.copy()
        final_groups = {}
        for title, items in merged.items():
            # tenta encontrar se title corresponde (case-insensitive) a alguma existing desc
            found_existing = None
            title_norm = self.correct_text(title).strip().lower()
            for norm_desc, (orig_desc, code) in norm_existing_map.items():
                if title_norm == norm_desc:
                    found_existing = (orig_desc, code)
                    break
            if found_existing:
                orig_desc, code = found_existing
                codes[orig_desc] = code
                # garante que o group key use a descri√ß√£o original do F17
                final_groups[orig_desc] = list(dict.fromkeys(items))
            else:
                # novo c√≥digo
                # evita criar duplicata se t√≠tulo j√° presente no mapa
                if title in codes:
                    assigned_code = codes[title]
                else:
                    assigned_code = next_code
                    codes[title] = assigned_code
                    next_code += 1
                final_groups[title] = list(dict.fromkeys(items))

        return codes, final_groups
    
    def standardize_with_chatgpt(self, phrase: str) -> str:
        """Padroniza frase usando ChatGPT (OpenAI) seguindo regras IPO"""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            return phrase
        client = get_openai_client(api_key=api_key)
        
        # Prompt mais espec√≠fico conforme regras IPO
        # Tenta carregar prompt do arquivo
        default_prompt = (
            f"Apply IPO rules to standardize this category title: '{phrase}'.\n"
            "Rules:\n"
            "1. Correct spelling and grammar.\n"
            "2. Capitalize the first letter (Sentence case).\n"
            "3. Keep it concise and descriptive.\n"
            "4. Do not change the meaning.\n"
            "5. Return ONLY the standardized text."
        )
        
        prompt = default_prompt
        try:
            base_dir = os.path.dirname(os.path.abspath(__file__))
            prompt_path = os.path.join(base_dir, 'prompts', 'standardization_prompt.txt')
            if os.path.exists(prompt_path):
                with open(prompt_path, 'r', encoding='utf-8') as f:
                    template = f.read()
                    prompt = template.replace('{phrase}', phrase)
        except Exception as e:
            print(f"Erro ao carregar prompt de padroniza√ß√£o: {e}. Usando padr√£o.")
        
        try:
            # Verifica cache antes de chamar
            cached_response = self.cache.get(prompt, "standardize")
            if cached_response:
                return cached_response

            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            # Se chegou aqui, a API respondeu corretamente
            try:
                content = response.choices[0].message.content.strip()
                # Remove aspas se o modelo adicionar
                if content.startswith('"') and content.endswith('"'):
                    content = content[1:-1]
            except Exception:
                content = str(response.choices[0].message.get('content', '')).strip()
            
            # Salva no cache
            self.cache.set(content, prompt, "standardize")
            
            # marca disponibilidade
            try:
                self.chatgpt_available = True
            except Exception:
                pass
            return content
        except Exception:
            # marca indisponibilidade para evitar tentativas repetidas
            try:
                self.chatgpt_available = False
            except Exception:
                pass
            return phrase

    def create_detailed_report(self, codes: Dict[str, int], groups: Dict[str, List[str]], question_name: str, processing_method: str = None) -> str:
        """Cria relat√≥rio detalhado de agrupamentos"""
        
        report_lines = []
        report_lines.append(f"RELAT√ìRIO DE AGRUPAMENTOS ‚Äì {question_name.upper()}")
        report_lines.append("")
        
        # Adiciona informa√ß√£o sobre o m√©todo usado
        if processing_method:
            method_info = {
                'chatgpt': 'ü§ñ Processado com ChatGPT (OpenAI GPT-4o)',
                'fallback_local': 'üîß Processado com Agrupador Local (Fallback)'
            }
            report_lines.append(f"M√©todo de Processamento: {method_info.get(processing_method, processing_method)}")
            report_lines.append("")
        
        # Ordena c√≥digos, reservados por √∫ltimo
        def code_sort_key(item):
            code = item[1]
            if code in [77, 88, 99]:
                return (1, code)
            return (0, code)
        sorted_codes = sorted(codes.items(), key=code_sort_key)
        
        for description, code in sorted_codes:
            # S√≥ tenta padronizar via ChatGPT se soubermos que a API est√° dispon√≠vel.
            # Se n√£o estiver dispon√≠vel (ou ainda n√£o testada), usa a padroniza√ß√£o local.
            # Tamb√©m verifica se o m√©todo de processamento foi 'chatgpt'
            use_chatgpt = getattr(self, 'chatgpt_available', False) or (processing_method == 'chatgpt')
            
            if use_chatgpt:
                standardized_desc = self.standardize_with_chatgpt(description)
            else:
                standardized_desc = self.correct_text(description)
            responses_in_group = groups.get(description, [])
            
            report_lines.append(f"C√≥digo {code} ‚Äì {standardized_desc}:")
            
            if responses_in_group:
                for response in responses_in_group:
                    if pd.notna(response) and str(response).strip():
                        report_lines.append(f" - {self.correct_text(str(response))}")
            else:
                report_lines.append(f" - (respostas com c√≥digo {code})")
            
            report_lines.append("")
        
        return "\n".join(report_lines)
    
    def create_output_columns_detailed(self, original_responses: List[Any], 
                                     codes: Dict[str, int], groups: Dict[str, List[str]]) -> Tuple[List[Any], List[Any]]:
        """Cria colunas de sa√≠da com mapeamento detalhado"""
        
        # Cria mapeamento de resposta original para c√≥digo
        response_to_code = {}
        
        for group_desc, responses_list in groups.items():
            code = codes.get(group_desc)
            if code:
                for response in responses_list:
                    response_to_code[response] = code
        
        code_column = []
        response_column = []
        
        for response in original_responses:
            if pd.isna(response):
                code_column.append("")
                response_column.append("")
            else:
                # Se a resposta √© um c√≥digo reservado, repete
                try:
                    resp_num = int(str(response).strip())
                    if resp_num in codes.values():
                        code_column.append(resp_num)
                        response_column.append(response)
                        continue
                except Exception:
                    pass
                if response in response_to_code:
                    code_column.append(response_to_code[response])
                    response_column.append(response)
                else:
                    # Tenta encontrar por similaridade
                    found_code = None
                    for group_desc, code in codes.items():
                        if self.calculate_similarity(str(response), group_desc) >= 0.8:
                            found_code = code
                            break
                    if found_code:
                        code_column.append(found_code)
                        response_column.append(response)
                    else:
                        code_column.append("ERROR")
                        response_column.append(response)
        
        return code_column, response_column

    def group_with_chatgpt(self, responses: list, f17: list = None, questionario: str = None) -> dict:
        """Agrupa respostas usando o ChatGPT, seguindo o prompt IPO, retornando um dicion√°rio {titulo: [respostas]}"""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise Exception("OPENAI_API_KEY n√£o encontrada no .env")
        client = get_openai_client(api_key=api_key)
        # Usa o prompt fornecido pelo usu√°rio como system prompt para o ChatGPT
        # Carrega prompt do sistema
        default_system_prompt = """
You are a survey coding specialist for the Institute of Opinion Research (IPO). Your objective is to receive data and code survey responses, following the IPO's rules exactly.

CRITICAL: You MUST process EVERY SINGLE response provided. Do not skip any responses. Every response must appear in exactly one group.

Your work must be carried out in mandatory steps. Please confirm the completion of each step before proceeding to the next.

Your objective is to receive data and code survey question responses, following the IPO's rules exactly. You can work in two ways:
1. Receive 3 types of files: ‚Ä¢ Coding Database (Excel) ‚Äì Contains the responses and their codes (when they exist). ‚Ä¢ Form 17 (F17) ‚Äì The codebook, with a tab for each question, listing responses and their respective codes. ‚Ä¢ Questionnaire ‚Äì Shows the text of the questions and indicates if they are closed, semi-open, or open.
2. Receive only the question column from the Coding Database and the corresponding column from the F17, to code only the specific question requested.

Correctly identify the type of question based on the column content:
Closed Question: contains only numeric codes in the response column (e.g., 1,2,3...). Must NEVER be coded ‚Äî it is already complete.
Semi-open Question: contains some codes and some loose phrases. Create codes only for the new phrases; next code starts at 10 or follows sequence.
Open Question: contains only phrases/words except non-response codes (77,88,99). Search F17 for existing codes; otherwise create new starting at 10.

Mandatory Workflow Steps:
1. Count the total number of responses provided. You MUST process ALL of them.
2. Analyze the question and the F17 to understand existing categories.
3. Analyze ALL open responses, correct spelling, standardize words, identify equivalent meanings.
4. Group responses ONLY when meaning is identical or equivalent.
5. NEVER create duplicate codes for the same meaning.
6. If a response has a unique meaning, assign a unique code.
7. Never use generic grouping like "Others".
8. Mandatory text review: correct spelling/grammar while preserving meaning; capitalize descriptions; standardize names; remove duplicates.
9. If you detect different codes for the same response, mark it as "CHANGES TO (correct code)".
10. If response is out of context or illegible, label it as "MANUAL REVIEW".
11. VERIFY: Count all responses in your groups. The total MUST equal the number of responses provided.

F17 Rules and Critical Rules: follow exactly as described; do not change non-response codes (55,66,77,88,99); do not reorder/insert/delete rows in the database.

Mandatory Output Formatting: Always return two columns: Left: Code, Right: Response. The code column must be filled in every row. Maintain original database right column. Do not reorder or add rows. In F17, texts should be grammatically adjusted without duplicate codes.

Mandatory Outputs:
1. Excel Coding Database (.xlsx) with responses and codes.
2. Excel Form 17 (.xlsx) with codes and responses.
3. Grouping report in TXT explaining group decisions in the required structure.

Working Format: Confirm question type before coding. Group equivalent meanings. Justify groupings. Work exclusively based on files provided and rules above. REMEMBER: Process EVERY response provided.
"""
        system_prompt = default_system_prompt
        try:
            base_dir = os.path.dirname(os.path.abspath(__file__))
            prompt_path = os.path.join(base_dir, 'prompts', 'system_prompt.txt')
            if os.path.exists(prompt_path):
                with open(prompt_path, 'r', encoding='utf-8') as f:
                    system_prompt = f.read()
        except Exception as e:
            print(f"Erro ao carregar prompt do sistema: {e}. Usando padr√£o.")
        f17_block = ""
        if f17:
            f17_block = "F17 (codebook):\n" + "\n".join([str(x) for x in f17])
        respostas_block = "\n".join([f"{i+1}. {str(x)}" for i, x in enumerate(responses)])
        total_respostas = len(responses)
        # User content pede explicitamente que o modelo retorne uma lista de objetos com codigo/titulo/respostas
        user_content = (
            f"{f17_block}\n\n"
            f"Total de respostas para processar: {total_respostas}\n"
            f"Responses to be coded (do not reorder rows, process ALL {total_respostas} responses):\n{respostas_block}\n\n"
            "CRITICAL REQUIREMENTS:"
            " 1. You MUST process ALL unique responses listed above."
            " 2. Create a Codebook that covers EVERY single response."
            " 3. Use existing F17 codes when a response matches exactly or closely. You MUST return these groups too."
            " 4. If you create new codes, start at 10 or the next available number."
            " 5. Return a JSON array (list) of objects with the exact fields:"
            " [{\"codigo\": <integer>, \"titulo\": <string>, \"respostas\": [<string>, ...]}, ...]."
            " 6. IMPORTANT: Even if a response matches an existing F17 code, you MUST include it in the output JSON with that code."
            " 7. Do NOT skip any response. The goal is to map every input to a code."
        )
        import sys
        import sys
        try:
            # Verifica cache para o agrupamento principal
            # Usa system_prompt e user_content como chave
            cached_grouping = self.cache.get(system_prompt, user_content, "grouping")
            
            if cached_grouping:
                print("[DEBUG] Usando resposta em CACHE para agrupamento!", flush=True)
                # Simula estrutura de resposta para o restante do c√≥digo processar
                # Precisamos retornar o conte√∫do como se fosse o 'content' extra√≠do
                content = cached_grouping
                # Pula a chamada da API
            else:
                print("[DEBUG] Chamando ChatGPT (function-calling)...", flush=True)
                # Define schema para function-calling: lista de objetos {codigo,titulo,respostas}
                functions = [
                    {
                        "name": "return_groups",
                        "description": "Retorna uma lista de grupos codificados seguindo o formato IPO",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "groups": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "codigo": {"type": "integer"},
                                            "titulo": {"type": "string"},
                                            "respostas": {"type": "array", "items": {"type": "string"}}
                                        },
                                        "required": ["codigo", "titulo", "respostas"]
                                    }
                                }
                            },
                            "required": ["groups"]
                        }
                    }
                ]

                # Tenta chamar com function-calling; alguns clientes legados podem rejeitar o par√¢metro
                try:
                    # Nova API OpenAI usa 'tools' ao inv√©s de 'functions'
                    try:
                        response = client.chat.completions.create(
                            model="gpt-4o",
                            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_content}],
                            temperature=0,
                            tools=[{
                                "type": "function",
                                "function": functions[0]
                            }],
                            tool_choice="auto"
                        )
                    except (TypeError, AttributeError):
                        # Tenta com 'functions' (API antiga)
                        try:
                            response = client.chat.completions.create(
                                model="gpt-4o",
                                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_content}],
                                temperature=0,
                                functions=functions,
                                function_call="auto"
                            )
                        except (TypeError, AttributeError):
                            # Fallback para chamada normal sem function-calling
                            print("[DEBUG] Function-calling n√£o suportado, usando chamada normal", flush=True)
                            response = client.chat.completions.create(
                                model="gpt-4o",
                                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_content}],
                                temperature=0
                            )
                except Exception as api_error:
                    error_str = str(api_error)
                    # Trata erros espec√≠ficos da API OpenAI
                    if '429' in error_str or 'insufficient_quota' in error_str or 'quota' in error_str.lower():
                        error_msg = "Quota da API OpenAI excedida. Verifique seus cr√©ditos e limite de uso em https://platform.openai.com/account/billing"
                        print(f"[DEBUG] {error_msg}", flush=True)
                        raise Exception(error_msg)
                    elif '401' in error_str or 'invalid_api_key' in error_str or 'authentication' in error_str.lower():
                        error_msg = "Chave de API OpenAI inv√°lida ou expirada. Verifique sua chave em https://platform.openai.com/api-keys"
                        print(f"[DEBUG] {error_msg}", flush=True)
                        raise Exception(error_msg)
                    elif 'rate_limit' in error_str.lower() or 'too_many_requests' in error_str.lower():
                        error_msg = "Limite de requisi√ß√µes excedido. Aguarde alguns minutos e tente novamente."
                        print(f"[DEBUG] {error_msg}", flush=True)
                        raise Exception(error_msg)
                    else:
                        error_msg = f"Erro na chamada √† API OpenAI: {error_str}"
                        print(f"[DEBUG] {error_msg}", flush=True)
                        raise Exception(error_msg)
                    print("[DEBUG] ChatGPT respondeu!", flush=True)
                    self.chatgpt_available = True # Marca como dispon√≠vel ap√≥s sucesso
                import json
                # nova resposta: acesso via response.choices[0].message.content ou via response.choices[0].message['content']
                # adaptamos para ambos formatos
                # Salva raw response para auditoria
                try:
                    raw_path = os.path.join(os.getenv('RESULTS_FOLDER', '/tmp/ipo_results'), f"raw_chatgpt_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                    with open(raw_path, 'w', encoding='utf-8') as rf:
                        try:
                            # tenta serializar o objeto de resposta diretamente
                            import json as _json
                            _json.dump(response.__dict__ if hasattr(response, '__dict__') else str(response), rf, ensure_ascii=False, indent=2)
                        except Exception:
                            rf.write(str(response))
                    print(f"[DEBUG] Raw ChatGPT salvo em: {raw_path}", flush=True)
                except Exception:
                    pass

                # Extrai conte√∫do: verifica tools (nova API) ou function_call (API antiga) ou content direto
                content = None
                try:
                    msg = response.choices[0].message
                    
                    # Nova API: verifica 'tool_calls' primeiro
                    if hasattr(msg, 'tool_calls') and msg.tool_calls:
                        for tool_call in msg.tool_calls:
                            if hasattr(tool_call, 'function') and hasattr(tool_call.function, 'arguments'):
                                content = tool_call.function.arguments
                                print("[DEBUG] Conte√∫do extra√≠do de tool_calls (nova API)", flush=True)
                                break
                    
                    # Se n√£o encontrou em tool_calls, tenta function_call (API antiga)
                    if not content:
                        if isinstance(msg, dict) and 'function_call' in msg and msg['function_call']:
                            func = msg['function_call']
                            content = func.get('arguments') or func.get('args') or ''
                            if content:
                                print("[DEBUG] Conte√∫do extra√≠do de function_call (dict)", flush=True)
                        else:
                            # objeto com atributos
                            try:
                                fc = getattr(msg, 'function_call', None)
                                if fc:
                                    content = fc.get('arguments') if isinstance(fc, dict) else getattr(fc, 'arguments', None)
                                    if content:
                                        print("[DEBUG] Conte√∫do extra√≠do de function_call (attr)", flush=True)
                            except Exception:
                                pass
                    
                    # Se ainda n√£o encontrou, tenta content direto
                    if not content:
                        content = getattr(msg, 'content', None) or (msg.get('content') if isinstance(msg, dict) else None)
                        if content:
                            print("[DEBUG] Conte√∫do extra√≠do de message.content", flush=True)
                            
                except Exception as e_extract:
                    print(f"[DEBUG] Erro ao extrair conte√∫do: {e_extract}", flush=True)
                    # fallback para estruturas antigas
                    try:
                        content = response.choices[0].message.content
                    except Exception:
                        try:
                            content = response.choices[0].message['content'] if isinstance(response.choices[0].message, dict) else str(response)
                        except Exception:
                            content = str(response)
                
                # Salva no cache se tiver conte√∫do v√°lido
                if content and isinstance(content, str) and content.strip():
                    self.cache.set(content, system_prompt, user_content, "grouping")
            
            if not content or (isinstance(content, str) and not content.strip()):
                error_msg = "ChatGPT retornou resposta sem conte√∫do v√°lido. Verifique a resposta da API."
                print(f"[DEBUG] {error_msg}", flush=True)
                # print(f"[DEBUG] Tipo de resposta: {type(response)}", flush=True)
                # print(f"[DEBUG] Estrutura da mensagem: {dir(response.choices[0].message) if response.choices else 'sem choices'}", flush=True)
                raise Exception(error_msg)

            print(f"[DEBUG] Conte√∫do bruto retornado pelo ChatGPT:\n{content}", flush=True)
            # Constr√≥i mapa de existing_codes a partir do f17 para prioriza√ß√£o (se fornecido)
            existing_codes = {}
            if f17:
                for line in f17:
                    try:
                        parts = str(line).split('|', 1)
                        if len(parts) == 2:
                            code = int(parts[0].strip())
                            desc = parts[1].strip()
                            existing_codes[desc] = code
                    except Exception:
                        continue

            # Tenta extrair JSON da resposta de forma tolerante
            grupos = None
            # busca o primeiro e √∫ltimo colchete/brace que pare√ßam envelopar um JSON
            possible_jsons = []
            # procura por { ... }
            starts = [m.start() for m in re.finditer(r'\{', content)]
            ends = [m.start() for m in re.finditer(r'\}', content)]
            if starts and ends:
                for s in starts:
                    for e in reversed(ends):
                        if e > s:
                            candidate = content[s:e+1]
                            possible_jsons.append(candidate)
                            break
            # tamb√©m tenta arrays [...]
            starts_b = [m.start() for m in re.finditer(r'\[', content)]
            ends_b = [m.start() for m in re.finditer(r'\]', content)]
            if starts_b and ends_b:
                for s in starts_b:
                    for e in reversed(ends_b):
                        if e > s:
                            candidate = content[s:e+1]
                            possible_jsons.append(candidate)
                            break

            parsed = False
            for candidate in possible_jsons:
                try:
                    grupos = json.loads(candidate)
                    parsed = True
                    break
                except Exception as e_json:
                    # ignora e tenta pr√≥ximo
                    print(f"[DEBUG] json.loads falhou para candidato (len={len(candidate)}): {e_json}", flush=True)
                    continue
            if not parsed:
                # √∫ltima tentativa: tenta carregar todo o conte√∫do bruto se ele for um JSON v√°lido
                try:
                    grupos = json.loads(content)
                    parsed = True
                except Exception as e_json:
                    print(f"[DEBUG] N√£o foi poss√≠vel parsear JSON do conte√∫do retornado pelo ChatGPT: {e_json}", flush=True)
                    grupos = None
            # Agora esperamos que 'grupos' seja uma lista de objetos: [{codigo, titulo, respostas}, ...]
            # OU um dicion√°rio com a chave 'groups': {'groups': [{codigo, titulo, respostas}, ...]}
            # OU um √∫nico objeto de grupo: {codigo, titulo, respostas} (caso raro, mas poss√≠vel)
            
            if isinstance(grupos, dict):
                if 'groups' in grupos:
                    # ChatGPT retornou {'groups': [...]} - extrai a lista
                    grupos = grupos['groups']
                    print("[DEBUG] Extra√≠do 'groups' do dicion√°rio retornado pelo ChatGPT", flush=True)
                elif 'codigo' in grupos and 'titulo' in grupos and 'respostas' in grupos:
                    # ChatGPT retornou um √∫nico grupo sem lista
                    print("[DEBUG] ChatGPT retornou um √∫nico grupo n√£o envelopado. Convertendo para lista.", flush=True)
                    grupos = [grupos]
                else:
                    # Tenta encontrar lista em outras chaves
                    found_list = False
                    for key in ['data', 'result', 'items']:
                        if key in grupos and isinstance(grupos[key], list):
                            grupos = grupos[key]
                            print(f"[DEBUG] Extra√≠do lista da chave '{key}'", flush=True)
                            found_list = True
                            break
                    
                    if not found_list:
                        # √â um dicion√°rio mas n√£o tem estrutura conhecida
                        error_msg = f"Formato inesperado do retorno do ChatGPT (esperava lista ou dict com 'groups'): {type(grupos)} -> {list(grupos.keys()) if grupos else 'vazio'}"
                        print(f"[DEBUG] {error_msg}", flush=True)
                        raise Exception(error_msg)

            if isinstance(grupos, list):
                codes = {}
                groups_map = {}
                for item in grupos:
                    if not isinstance(item, dict):
                        print(f"[DEBUG] Item ignorado (n√£o √© dict): {item}", flush=True)
                        continue
                    if 'codigo' in item and 'titulo' in item and 'respostas' in item:
                        try:
                            codigo = int(item['codigo'])
                        except Exception:
                            # pula itens com codigo inv√°lido
                            print(f"[DEBUG] Codigo inv√°lido no item: {item}", flush=True)
                            continue
                        titulo = self.correct_text(str(item['titulo']))
                        respostas_list = [r for r in item.get('respostas', []) if isinstance(r, str) and r.strip()]
                        codes[titulo] = codigo
                        groups_map[titulo] = respostas_list
                    else:
                        print(f"[DEBUG] Item sem campos esperados: {item}", flush=True)
                        continue
                # normaliza e une t√≠tulos semelhantes
                groups_map = self.merge_similar_groups(groups_map, threshold=85)
                # garante que n√£o haja c√≥digos duplicados para t√≠tulos iguais: se houver conflito, prioriza c√≥digos do F17
                final_codes = {}
                for titulo, respostas in groups_map.items():
                    # se titulo corresponde a existing_codes, use o c√≥digo existente
                    title_norm = self.correct_text(titulo).strip().lower()
                    used_code = None
                    for desc, code in existing_codes.items():
                        if self.correct_text(desc).strip().lower() == title_norm:
                            used_code = code
                            break
                    if used_code is None:
                        used_code = codes.get(titulo, None)
                    if used_code is None:
                        # atribui novo c√≥digo sequencial
                        max_existing = max([c for c in existing_codes.values()] + [9])
                        used_code = max_existing + 1
                    final_codes[titulo] = used_code
                if final_codes and groups_map:
                    # Verifica se todas as respostas foram processadas
                    total_respostas_mapeadas = sum(len(resps) for resps in groups_map.values())
                    total_respostas_originais = len(responses)
                    
                    print(f"[DEBUG] ‚úÖ ChatGPT retornou {len(final_codes)} grupos v√°lidos", flush=True)
                    print(f"[DEBUG] Respostas mapeadas: {total_respostas_mapeadas} de {total_respostas_originais} originais", flush=True)
                    
                    if total_respostas_mapeadas < total_respostas_originais:
                        faltando = total_respostas_originais - total_respostas_mapeadas
                        print(f"[DEBUG] ‚ö†Ô∏è ATEN√á√ÉO: {faltando} respostas n√£o foram processadas pelo ChatGPT!", flush=True)
                        print(f"[DEBUG] Respostas originais: {responses[:10]}...", flush=True)
                        print(f"[DEBUG] Respostas mapeadas: {[r for resps in groups_map.values() for r in resps[:10]]}...", flush=True)
                        # N√£o lan√ßa exce√ß√£o, mas avisa - o sistema vai tentar mapear as faltantes depois
                    
                    return final_codes, groups_map
                else:
                    print(f"[DEBUG] ‚ö†Ô∏è ChatGPT retornou grupos vazios ap√≥s processamento", flush=True)
                    raise Exception("ChatGPT retornou grupos vazios ap√≥s processamento. Verifique o formato da resposta.")
            else:
                error_msg = f"Formato inesperado do retorno do ChatGPT (esperava lista): {type(grupos)} -> {grupos}"
                print(f"[DEBUG] {error_msg}", flush=True)
                print(f"[DEBUG] Conte√∫do bruto recebido: {content[:500]}...", flush=True)
                raise Exception(f"{error_msg}. Conte√∫do recebido: {str(content)[:200]}")
        except Exception as e:
            error_msg = f"Erro ao agrupar com ChatGPT: {str(e)}"
            print(f"[DEBUG] {error_msg}", flush=True)
            if 'content' in locals():
                print(f"[DEBUG] Conte√∫do retornado pelo ChatGPT: {content[:500]}...", flush=True)
            try:
                self.chatgpt_available = False
            except Exception:
                pass
            # Re-lan√ßa a exce√ß√£o ao inv√©s de retornar vazio
            raise Exception(error_msg)

    def merge_similar_groups(self, grupos: dict, threshold: int = 85) -> dict:
        """Une grupos com t√≠tulos muito semelhantes usando fuzzy matching."""
        keys = list(grupos.keys())
        merged = {}
        used = set()

        # Pr√©-calc normalizado para chaves
        norm_map = {k: self.normalize_text(k) for k in keys}

        for i, k1 in enumerate(keys):
            if k1 in used:
                continue
            merged[k1] = list(grupos[k1])
            for j, k2 in enumerate(keys):
                if i == j or k2 in used:
                    continue
                n1 = norm_map.get(k1, '')
                n2 = norm_map.get(k2, '')
                # mescla se uma forma for substring da outra (ex: 'posto saude' vs 'posto de saude')
                if n1 and n2 and (n1 in n2 or n2 in n1):
                    merged[k1].extend(grupos[k2])
                    used.add(k2)
                    continue
                sim = fuzz.token_set_ratio(n1, n2)
                if sim >= threshold:
                    merged[k1].extend(grupos[k2])
                    used.add(k2)
            used.add(k1)
        # remove duplicatas em cada grupo e retorna
        for k in list(merged.keys()):
            merged[k] = list(dict.fromkeys(merged[k]))
        return merged

